---
title: "Generating severity scores"
author: "Laura Cosgrove"
date: "11/20/2018"
output: github_document
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(RPostgreSQL)
library(tidyverse)
library(dbplyr)
```


```{r dbconnect}
# Load configuration settings
dbdriver <- 'PostgreSQL'
host  <- '127.0.0.1'
port  <- '5432'
user  <- 'postgres'
password <- 'postgres'
dbname <- 'mimic'
schema <- 'mimiciii'
# Connect to the database using the configuration settings
con <- dbConnect(dbDriver(dbdriver), dbname = dbname, host = host, port = port, 
                 user = user, password = password)
# Set the default schema
dbExecute(con, paste("SET search_path TO ", schema, sep=" "))
```

Set this database as the connection for all future sql chunks:

```{r}
knitr::opts_chunk$set(connection = "con")
```

The above chunk works if you use knitr to generate the analysis! But in building the analysis it is maybe better to run your query by saving the sql query as a character object then using `dbGetQuery`.

Credit for SQL code authoring is not mine:

We need to generate some views into the database before the query to generate the severity scores works.
--  1) uofirstday - generated by urine-output-first-day.sql
--  2) ventdurations - generated by ventilation-durations.sql
--  3) vitalsfirstday - generated by vitals-first-day.sql
--  4) gcsfirstday - generated by gcs-first-day.sql
--  5) labsfirstday - generated by labs-first-day.sql
--  6) bloodgasarterialfirstday - generated by blood-gas-first-day-arterial.sql

```{r, eval = FALSE}
urine_view <- read_file("./database/mimic-code/concepts/firstday/urine-output-first-day.sql")
ventdurations_view <- read_file("./database/mimic-code/concepts/durations/ventilation-durations.sql")
vitals_view <- read_file("./database/mimic-code/concepts/firstday/vitals-first-day.sql")
gcs_view <- read_file("./database/mimic-code/concepts/firstday/gcs-first-day.sql")
labs_view <- read_file("./database/mimic-code/concepts/firstday/labs-first-day.sql")
bloodgasarterial_view <- read_file("./database/mimic-code/concepts/firstday/blood-gas-first-day-arterial.sql")
sapsii_view <- read_file("./database/mimic-code/concepts/severityscores/sapsii.sql")


#Generate materialized views
dbGetQuery(con, urine_view)
dbGetQuery(con, ventdurations_view)
dbGetQuery(con, vitals_view)
dbGetQuery(con, gcs_view)
dbGetQuery(con, labs_view)
dbGetQuery(con, bloodgasarterial_view)
dbGetQuery(con, sapsii_view)

```

Rewrite `sapsii_view` in R using `dbplyr`?

```{r}

```


```{r view}
#View sapsii_data
sapsii_query <- "SELECT *
              FROM sapsii i;"
sapsii_data <- as.tibble(dbGetQuery(con, sapsii_query))
write_csv(sapsii_data, path = "./database/sapsii.csv")

```

OMG Finally!

Plot distribution:

```{r}
sapsii_data %>% 
  ggplot(aes(x = sapsii)) + 
  geom_histogram()
```

A note in the SQL file is the following: 
Note:
The score is calculated for *all* ICU patients, with the assumption that the user will subselect appropriate ICUSTAY_IDs.
For example, the score is calculated for neonates, but it is likely inappropriate to actually use the score values for these patients.

```{sql connection=con}
SELECT *
FROM sapsii i
```


### Visualize how fractions of death increase 

```{r}
admissions <- read_csv("./database/data/ADMISSIONS.csv.gz") %>% 
  janitor::clean_names()
patients <- read_csv("./database/data/PATIENTS.csv.gz") %>% 
  janitor::clean_names()
sapsii <- read_csv("./database/sapsii.csv") %>% 
  janitor::clean_names()


admissions %>% 
  inner_join(., patients, by = "subject_id") %>% 
  filter(has_chartevents_data == 1) %>% 
  inner_join(., sapsii, by = "hadm_id") %>% 
  mutate(target = if_else(deathtime %in% NA, 0, 1),
         predictor = sapsii) %>%
  select(subject_id.x, target, predictor) %>% 
  group_by(predictor) %>% 
  summarize(deaths = sum(target),
            n = n()) %>% 
  mutate(frac_deaths = deaths/n) %>% 
  ggplot(aes(x = predictor, y = frac_deaths)) +
  geom_point(aes(color = n)) + 
  labs(x = "SAPS II Score",
       y = "Mortality Fraction", 
       title = "Predicting Mortality of ICU Patients with First-Day SAPS II scores") +
  theme_bw()
  

```

Repeat for other severity scores:

(Note: must run document once before knitting, but eval is set to false so the views aren't re-loaded every time you knit.)
```{r, eval = FALSE}
sofa_view <- read_file("./database/mimic-code/concepts/severityscores/sofa.sql")
lods_view <- read_file("./database/mimic-code/concepts/severityscores/lods.sql")
saps_view <- read_file("./database/mimic-code/concepts/severityscores/saps.sql")
apsiii_view <- read_file("./database/mimic-code/concepts/severityscores/apsiii.sql")
oasis_view <- read_file("./database/mimic-code/concepts/severityscores/oasis.sql")

#SOFA needs echo data 
echodata_view <- read_file("./database/mimic-code/concepts/echo-data.sql")

dbGetQuery(con, echodata_view)
dbGetQuery(con, sofa_view)

#LODS
dbGetQuery(con, lods_view)

#SAPS needs ventilated first day
ventfirstday_view <- read_file("./database/mimic-code/concepts/firstday/ventilation-first-day.sql")
dbGetQuery(con, ventfirstday_view)
dbGetQuery(con, saps_view)

# APSIII  
dbGetQuery(con, apsiii_view)

# OASIS  
dbGetQuery(con, oasis_view)

```

As before, read all the data from the generated materialized views into tibbles:

```{r}
#SOFA
sofa_query <- "SELECT *
              FROM sofa i;"
sofa_data <- as.tibble(dbGetQuery(con, sofa_query))

#LODS
lods_query <- "SELECT *
              FROM lods i;"
lods_data <- as.tibble(dbGetQuery(con, lods_query))

#SAPS
saps_query <- "SELECT *
              FROM saps i;"
saps_data <- as.tibble(dbGetQuery(con, saps_query))

# APSIII  
apsiii_query <- "SELECT *
              FROM apsiii i;"
apsiii_data <- as.tibble(dbGetQuery(con, apsiii_query))

#OASIS
oasis_query <- "SELECT *
              FROM oasis i;"
oasis_data <- as.tibble(dbGetQuery(con, oasis_query))
```

Plot curves for all other scores

```{r}
# I should create a nested df where I can map the inner join and generate multiple plots in the same code chunk

admissions %>% 
  inner_join(., patients, by = "subject_id") %>% 
  filter(has_chartevents_data == 1) %>% 
  inner_join(., sofa_data, by = "hadm_id") %>% 
  mutate(target = if_else(deathtime %in% NA, 0, 1),
         predictor = sofa) %>%
  select(subject_id.x, target, predictor) %>% 
  group_by(predictor) %>% 
  summarize(deaths = sum(target),
            n = n()) %>% 
  mutate(frac_deaths = deaths/n) %>% 
  ggplot(aes(x = predictor, y = frac_deaths)) +
  geom_point(aes(color = n)) + 
  theme_bw()

# Make a big datasheet?
all_scores <- admissions %>% 
  inner_join(., patients, by = "subject_id") %>% 
  inner_join(., sapsii_data, by = "hadm_id") %>% 
  inner_join(., sofa_data, by = "hadm_id") %>% 
  inner_join(., lods_data, by = "hadm_id") %>% 
  inner_join(., saps_data, by = "hadm_id") %>% 
  inner_join(., apsiii_data, by = "hadm_id") %>% 
  inner_join(., oasis_data, by = "hadm_id") %>% 
  mutate(target = if_else(deathtime %in% NA, 0, 1)) %>% 
  select(subject_id.x, hadm_id, target, sapsii, saps, sofa, lods, apsiii, oasis)

all_scores %>% 
  select(sapsii:oasis) %>% 
  cor()

```

```{r plot, fig.width= 12 , fig.asp = 0.4}
all_scores %>% 
  distinct(hadm_id, .keep_all = TRUE) %>% 
  gather(key = score, value, sapsii, saps, sofa, lods, apsiii, oasis) %>% 
  group_by(score, value) %>% 
  summarize(deaths = sum(target), 
            n = n()) %>% 
  mutate(frac_deaths = deaths/n) %>% 
  ggplot(aes(x = value, y = frac_deaths)) + 
  geom_point(aes(color = n)) +
  facet_grid(~score)

```

## Individual mortality prediction

It's fine just as a quick gut check to see how fraction of deaths increase over the distributions of the various severity scores. But how do the scores perform based on their original authored likelihoods?

### SAPS II

We'll start with the SAPS II score. 

Individual mortality prediction for the SAPS II score is defined by its authors to be: 

log([pr(death)][1 - pr(death)]) = -7.7631 + 0.07237*SAPSII + 0.9971*log(1 + SAPSII)

A mortality prediction algorithm is said to have adequate discrimination if it tends to assign higher severity scores to patients that died in the hospital compared to those that did not. To evaluate discrimination, we'll visualize the probability of death as predicted by the SAPSII score versus the actual proportion of patients who died with that SAPSII score.

```{r}
sapsii_data %>% 
  mutate(prob_death = exp(-7.7631 + 0.07237*sapsii + 0.9971*log(1 + sapsii))/(1 + exp(-7.7631 + 0.07237*sapsii + 0.9971*log(1 + sapsii)))) %>% 
  inner_join(admissions, by = "hadm_id") %>% 
  mutate(mortality = if_else(deathtime %in% NA, 0, 1),
         hosp_los = admittime %--% dischtime) %>% 
  select(hadm_id, sapsii, prob_death, hosp_los, mortality, starts_with("admission"), diagnosis) %>% 
  group_by(sapsii) %>% 
  add_tally(mortality) %>% 
  rename(mortality_by_group = n) %>% 
  add_tally() %>% 
  mutate(prop_death = mortality_by_group/n) %>% 
  select(sapsii, prob_death, prop_death) %>% 
  ggplot(aes(x = prob_death, y = prop_death)) + 
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
<<<<<<< HEAD
  labs(title = "SAPSII",
      x = "Probability of Death from Mainterm Regression",
       y = "True Proportion of Deaths") + 
  theme_bw()
```

We see a better fit with the mainterm logistic regression, which makes sense given that the literature value was an externally-generated prediction, while our regression is internally-generated. Keep that caveat in mind as we continue with algorithm comparison for other scores, because no direct external value exists for predictive capability of the other severity scores; rather, they're used in practive as clinical decision support rather than giving probability determination.


## SOFA Score

Authors of the SOFA score do not publish a base probability calculation for likelihood of death, so I'll use a main-term logistic regression to obtain mortality prediction based on the SOFA score.

```{r}
fit_sofa <- predictor_detail_data %>% 
  distinct(icustay_id, .keep_all = TRUE) %>% 
  select(sofa, death_bin) %>% 
  glm(death_bin ~ sofa, family = binomial, data = .) 

fit_sofa %>% 
  broom::tidy()

fit_sofa %>% 
  broom::tidy(conf.int = TRUE, exponentiate = TRUE)

```

The regression fits a parameter estimate of log([pr(death)][1 - pr(death)]) = -3.36 + 0.273*SOFA; for every unit increase in SOFA score, you can expect to have 1.31x the odds of death.


```{r}
predictor_detail_data %>% 
  distinct(icustay_id, .keep_all = TRUE) %>% 
  select(hadm_id, sofa, death_bin) %>% 
  mutate(prob_death = exp(-3.36 + 0.273*sofa)/(1 + exp(-3.36 + 0.273*sofa))) %>% 
  group_by(sofa) %>% 
  add_tally(death_bin) %>% 
  rename(tot_death_by_group = n) %>% 
  add_tally() %>% 
  mutate(prop_death = tot_death_by_group/n) %>% 
  select(sofa, prob_death, prop_death)%>% 
  ggplot(aes(x = prob_death, y = prop_death)) + 
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
  labs(x = "Probability of Death from Mainterm Regression",
       y = "True Proportion of Deaths") + 
  theme_bw()
```

This regression obtains a good fit, which makes sense because the probability was obtained from our data rather than a literature value. 

## Map for all Scores

We can use a `map` operation with some tidying of our original data to speed up the generation of the predicted probabilities of each score.

```{r lods}
predictor_detail_data_tidy <- predictor_detail_data %>% 
  distinct(icustay_id, .keep_all = TRUE) %>% 
  gather(key = score, value = score_value, sapsii, sofa, lods, apsiii, oasis) %>% 
  select(icustay_id, score, score_value, everything())

predictor_detail_data_tidy %>% 
  select(score, death_bin, score_value) %>% 
  group_by(score) %>% 
  nest() %>% 
  mutate(glm = map(data, ~glm(death_bin ~ score_value, family = binomial, data = .))) %>% 
  mutate(glm_coef = map(glm, broom::tidy)) %>% 
  select(score, glm_coef) %>% 
  unnest() %>% 
  knitr::kable()
```

All severity scores are significant predictors. We'll need to impute a special result for SAPSII following this code, because we're taking the literature value:

```{r }

score_data_tidy <- predictor_detail_data_tidy %>% 
  select(score, death_bin, score_value) %>% 
  group_by(score) %>% 
  nest() %>% 
  mutate(glm = map(data, ~glm(death_bin ~ score_value, family = binomial, data = .))) %>% 
  mutate(glm_coef = map(glm, broom::tidy)) %>% 
  mutate(predictions = map2(data, glm, modelr::add_predictions)) %>% 
  select(score, predictions) %>% 
  unnest() %>% 
  mutate(prob_death = exp(pred)/(1 + exp(pred))) %>% 
  mutate(prob_death = if_else(score == "sapsii", 
                              exp(-7.7631 + 0.07237*score_value + 0.9971*log(1 + score_value))/(1 + exp(-7.7631 +  0.07237*score_value + 0.9971*log(1 + score_value))), 
                              prob_death))
```

```{r plot 2, fig.width= 10, cache=TRUE}
score_data_tidy %>% 
  select(score, death_bin, prob_death) %>% 
  group_by(score, prob_death) %>%
  add_tally(death_bin) %>% 
  rename(tot_death_by_group = n) %>% 
  add_tally() %>% 
  mutate(prop_death = tot_death_by_group/n) %>% 
  ggplot(aes(x = prob_death, y = prop_death, color = score)) + 
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
  labs(x = "Probability of Death from Mainterm Regression or Literature Value",
       y = "True Proportion of Deaths")

```

Above, we've plotted the true, observed proportion of deaths for each score value versus the probability of death predicted from mainterm regression (or, in the SAPSII case, a literature value) for each severity score. Each point is a particular score value for a patient -- i.e., there are 5x the numbers of ICU patients plotted.

There's some interesting data here!  As expected, the SAPSII test, being from a literature value, is further under the equality line for most scores. This means that the external, literature value is more aggressive in estimating probability of death for patients, compared ot probabilities imputed from regression on our data. This could be because the care at Beth Israel Deaconness is unusually good, or it could be because it is clinically useful that a classification algorithm be a bit more aggressive in estimating severity.

Another interesting finding is that some severity scores "silo" near probabilities 1 and 0. This could mean that there needs to be an adjustment in the range of the severity scores.

## Area under ROC curves

To plot the area under the curve for the SAPSII and SOFA scores, I need to calculate the respective TPR and FPR for each score. I'll use the literature value for the SAPSII score.

```{r auc}
for_roc <- score_data_tidy %>% 
  select(score, death_bin, prob_death) %>% 
  group_by(score) %>% 
  nest()

###############ROC AUC Functions#############################
#This code could be improved with a better function.

roc_log_fcn <- function(result, y_prob){
  probs <- seq(0,1, by = 0.005)
  roc_log <- matrix(0, nrow = length(probs), ncol=2)
  i <- 1
  for(p in probs){
    pred <- y_prob > p
    ##False positive rate
    FPR <- sum(!result & pred)/sum(!result)
    ##True positive rate
    TPR <- sum(result & pred)/sum(result)
    roc_log[i,] <- c(FPR, TPR)
    i <- i + 1
  }
  return(roc_log)
}

#SAPSII
sapsii_for_roc <- for_roc %>% 
  filter(score == "sapsii") %>% 
  unnest() 
roc_log_sapsii <- roc_log_fcn(sapsii_for_roc$death_bin, sapsii_for_roc$prob_death)

#SOFA
sofa_for_roc <- for_roc %>% 
  filter(score == "sofa") %>% 
  unnest() 
roc_log_sofa <- roc_log_fcn(sofa_for_roc$death_bin, sofa_for_roc$prob_death)

#LODS
lods_for_roc <- for_roc %>% 
  filter(score == "lods") %>% 
  unnest() 
roc_log_lods <- roc_log_fcn(lods_for_roc$death_bin, lods_for_roc$prob_death)

#APSIII
apsiii_for_roc <- for_roc %>% 
  filter(score == "apsiii") %>% 
  unnest() 
roc_log_apsiii <- roc_log_fcn(apsiii_for_roc$death_bin, apsiii_for_roc$prob_death)

#OASIS
oasis_for_roc <- for_roc %>% 
  filter(score == "oasis") %>% 
  unnest() 
roc_log_oasis <- roc_log_fcn(oasis_for_roc$death_bin, oasis_for_roc$prob_death)


tidy_for_roc <- tibble(FPR_sapsii = roc_log_sapsii[,1], TPR_sapsii = roc_log_sapsii[,2],
       FPR_sofa = roc_log_sofa[,1], TPR_sofa = roc_log_sofa[,2],
       FPR_lods = roc_log_lods[,1], TPR_lods = roc_log_lods[,2],
       FPR_apsiii = roc_log_apsiii[,1], TPR_apsiii = roc_log_apsiii[,2],
       FPR_oasis = roc_log_oasis[,1], TPR_oasis = roc_log_oasis[,2]) %>% 
  gather(key = score, value = FPR, starts_with("FPR")) %>% 
  gather(key = score2, value = TPR, starts_with("TPR")) %>% 
  mutate(score = if_else(score == "FPR_sapsii", 
                         #yes sapsii
                         if_else(score2 == "TPR_sapsii", "sapsii", "NA"),
                         #no sapsii 
                            (if_else(score == "FPR_sofa", 
                              # yes sofa 
                              if_else(score2 == "TPR_sofa", "sofa", "NA"), 
                              # no sofa
                              (if_else(score == "FPR_lods",
                              #yes lods
                                if_else(score2 == "TPR_lods", "lods", "NA"),
                              # no lods
                              (if_else(score == "FPR_apsiii",
                                if_else(score2 == "TPR_apsiii", "apsiii", "NA"), 
                              (if_else(score == "FPR_oasis", 
                                if_else(score2 == "TPR_oasis", "oasis", "NA"), "NA"
                              )))))))))
                         ) %>% 
  filter(score != "NA")

tidy_for_roc %>% 
  select(score, FPR, TPR) %>% 
  ggplot(aes(x = FPR, y = TPR, color = score)) +
  geom_point() +
  geom_step() +
  labs(title = "ROC Curves") +
  theme_bw() +
  scale_color_viridis_d()

```

SAPSII looks like the best-performing model, and let's confirm that with an AUROC analysis.

```{r auroc }
auc <- function(roc){
  len <- nrow(roc)
  ##The "delta X" values
  delta <- roc[-1,1]-roc[-len,1]
  ##The "heights" the rectangle (drop the first or last).
  hgt <- roc[-1,2]
  ##The Riemann Sum
  sum(-delta*hgt)
}

tibble(score = c("sapsii", "sofa", "lods", "apsiii", "oasis"), AUROC = c(auc(roc_log_sapsii), auc(roc_log_sofa), auc(roc_log_lods), auc(roc_log_apsiii), auc(roc_log_oasis))) %>% 
  arrange(desc(AUROC)) %>% 
  knitr::kable()
```

SAPSII is our winner in terms of AUROC.

## Adding other predictors to our model

The above analysis showed us that when considered severity scores computed from biomarkers, the best mortality algorithm as imputed by an AUROC analysis was the SAPSII score, taking the literature values for probability.

Can we improve upon this severity score by adding other predictors?

Our initial analysis showed that the best-fit logistic regression model for mortaility in terms of AIC included the following covariate terms from the admissions dataset: admission_type, admission_location, insurance, religion, marital_status, and ethnicity.

```{r}
predictor_detail_data <- predictor_detail_data %>% 
  distinct(icustay_id, .keep_all = TRUE) %>% 
#Need a couple more variables
  inner_join(admissions, by = "hadm_id") %>% 
  select(icustay_id, death_bin, sapsii, admission_type.x, admission_age, admission_location, insurance, religion, marital_status, ethnicity.x) %>% 
  rename(admission_type = admission_type.x, ethnicity = ethnicity.x) %>% 
  mutate(admission_type = factor(admission_type), admission_age = factor(admission_age), insurance = factor(insurance), religion = factor(religion), marital_status = factor(marital_status), ethnicity = factor(ethnicity))

#Removing missing values for effective comparison
predictor_detail_data <- predictor_detail_data %>% 
  drop_na()

fit_null <- predictor_detail_data %>% 
  glm(death_bin ~ sapsii, data = .) 

fit_alt <- predictor_detail_data %>% 
  glm(death_bin ~ sapsii + admission_type + admission_location + insurance + religion + marital_status + ethnicity, family = binomial, data = .) 

#Use anova to compare the null with the added predictors model
anova(fit_null, fit_alt)

library(lmtest)
lrtest(fit_null, fit_alt)
##Shows a significant increase in log-likelihood
```

Our tests show a significant increase in log-likelihood for the alternative, larger model. 

We can calculate AUROC for our new model and compare against SAPSII. Note that we will be missing some values we previously had.

```{r}
 predictor_detail_data %>% 
  mutate(prob_death_sapsii = exp(-7.7631 + 0.07237*sapsii + 0.9971*log(1 + sapsii))/(1 + exp(-7.7631 +  0.07237*sapsii + 0.9971*log(1 + sapsii)))) %>% 
  modelr::add_predictions(fit_alt) %>% 
  mutate(prob_death_full = exp(pred)/(1 + exp(pred))) %>% 
  select(icustay_id, death_bin, prob_death_sapsii, prob_death_full) %>% 
  group_by(prob_death_sapsii) %>% 
  add_tally(death_bin) %>% 
  rename(tot_death_by_sapsii = n) %>% 
  add_tally() %>% 
  mutate(prop_death_sapsii = tot_death_by_sapsii/n) %>% 
  ungroup() %>% 
  select(death_bin, prob_death_sapsii, prob_death_full, prop_death_sapsii) %>% 
  group_by(prob_death_full) %>% 
  add_tally(death_bin) %>% 
  rename(tot_death_by_full = n) %>% 
  add_tally() %>% 
  mutate(prop_death_full = tot_death_by_full/n) %>% 
  select(prob_death_sapsii, prop_death_sapsii, prob_death_full, prop_death_full) %>% 
  ggplot() + 
  geom_point(aes(x = prob_death_sapsii, y = prop_death_sapsii), color = "blue") +
  geom_point(aes(x = prob_death_full, y = prop_death_full), color = "red") +
  geom_abline(slope = 1, intercept = 0) +
  labs(x = "Probability of Death from Mainterm Regression",
       y = "True Proportion of Deaths",
       caption = "Blue is SAPSII, red is with covariates")

=======
  labs(x = "Probability of Death ")
  
>>>>>>> 017d77e53d9b2f67b1a13bee768670353fff6558
  


  
```


